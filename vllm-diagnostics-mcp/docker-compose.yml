version: '3.8'

services:
  vllm-diagnostics:
    build: .
    ports:
      - "8001:8001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: python server.py
    
  # Example vLLM service (for testing)
  # Uncomment and modify as needed
  # vllm-server:
  #   image: vllm/vllm-openai:latest
  #   ports:
  #     - "8000:8000"
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]
  #   command: >
  #     --model meta-llama/Llama-2-7b-chat-hf
  #     --host 0.0.0.0
  #     --port 8000